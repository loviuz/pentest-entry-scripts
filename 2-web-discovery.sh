#!/bin/bash

target=$1

if [ -z $target ]
then
    echo 'Please specify a target!'
    exit
fi

report_path=reports/$target
mkdir -p $report_path

# 1) Scan the web server with nikto
rm $report_path/nikto.html
nikto -Display 1234EP -o $report_path/nikto.html -Format htm -Tuning 123bde -host $target

# Open scan
$browser file://$PWD/$report_path/nikto.html



# 2) Scan the web server for a list of juicy directories
gobuster -e -u http://$target -w wordlists/SecLists-master/Discovery/Web-Content/quickhits.txt > $report_path/directories.txt

# Open scan
$browser file://$PWD/$report_path/directories.txt



# 3) Scan the web server for DAV vulnerabilities
davtest -url http://$target > $report_path/davtest.txt

# Open scan
$browser file://$PWD/$report_path/davtest.txt




# 4) Scan the web server for useful entries in robots.txt
parsero -u http://$target > $report_path/parsero.txt

# Open scan
$browser file://$PWD/$report_path/parsero.txt




# 5) Spider all URLs and try to inject
blackwidow -u $target -s y -l 5 -o $report_path/blackwidow/ > $report_path/blackwidow.txt

# Unify and normalize all URLs found
#cat $report_path/blackwidow/*txt > $report_path/blackwidow/full_urls.txt
#for url in `cat $report_path/blackwidow/full_urls.txt`; do clean_url=`echo $url | sed -r 's/http:\/\///g; s/[\?]*C=[A-Z]{1};O=[A-Z]{1}//g ; s/\?\/$//g ; s/[\/]{2,}/\//g'`; echo 'http://'$clean_url >> $report_path/blackwidow/urls.txt; done
#rm $report_path/blackwidow/full_urls.txt

# Remove duplicates
#awk '!seen[$0]++' $report_path/blackwidow/urls.txt > $report_path/urls.txt
rm -R $report_path/blackwidow/


# Open scan
$browser file://$PWD/$report_path/blackwidow.txt


# Set permissions to other users
sudo chmod -R o+x $report_path