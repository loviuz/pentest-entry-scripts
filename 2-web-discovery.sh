#!/bin/bash

. ./config

report_path=reports/$host
mkdir -p $report_path

# 1) Scan the web server with nikto
rm $report_path/nikto.html
nikto -Display 1234EP -Option 'STATIC-COOKIE="$cookie"' -o $report_path/nikto.html -Format htm -Tuning 123bde -host $host

# Open scan
$browser file://$PWD/$report_path/nikto.html&



# 2) Scan the web server for a list of juicy directories
gobuster dir -c "$cookie" -e -u $protocol://$host$path -w "$wordlist" > $report_path/directories.txt

# Open scan
$browser file://$PWD/$report_path/directories.txt&



# 3) Scan the web server for DAV vulnerabilities
davtest -url $protocol://$host > $report_path/davtest.txt

# Open scan
$browser file://$PWD/$report_path/davtest.txt&




# 4) Scan the web server for useful entries in robots.txt
parsero -u $protocol://$host > $report_path/parsero.txt

# Open scan
$browser file://$PWD/$report_path/parsero.txt&




# 5) Spider all URLs and try to inject
blackwidow -c "$cookie" -u $protocol://$host$path -s y -l 5 -o $report_path/blackwidow/ > $report_path/blackwidow.txt

# Unify and normalize all URLs found
#cat $report_path/blackwidow/*txt > $report_path/blackwidow/full_urls.txt
#for url in `cat $report_path/blackwidow/full_urls.txt`; do clean_url=`echo $url | sed -r 's/http:\/\///g; s/[\?]*C=[A-Z]{1};O=[A-Z]{1}//g ; s/\?\/$//g ; s/[\/]{2,}/\//g'`; echo 'http://'$clean_url >> $report_path/blackwidow/urls.txt; done
#rm $report_path/blackwidow/full_urls.txt

# Remove duplicates
#awk '!seen[$0]++' $report_path/blackwidow/urls.txt > $report_path/urls.txt
rm -R $report_path/blackwidow/


# Open scan
$browser file://$PWD/$report_path/blackwidow.txt&


# Set permissions to other users
sudo chmod -R o+x $report_path